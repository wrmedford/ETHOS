# Model dimensions
vocab_size: 100277
d_model: 1024
num_layers: 16
num_heads: 16
num_dense_layers: 2
num_moe_layers: 14

# MLA parameters
q_lora_rank: 768
kv_lora_rank: 256
v_head_dim: 64
qk_nope_head_dim: 64
qk_rope_head_dim: 64

# MoE parameters
num_experts: 262144  # 512^2
d_latent: 128
d_intermediate_hypernet: 512
top_k: 16
num_routing_heads: 8
d_query: 512

# FFN
d_ffn_intermediate: 4096

# Other
max_seq_len: 4096
rms_norm_eps: 1e-6
rope_theta: 10000.0

# Training
batch_size: 8
learning_rate: 1e-4
num_epochs: 3
data_percentage: 0.01
use_triton: True