{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Training a Transformer with Low-Rank Expert Compression\n",
    "\n",
    "This notebook tests the trainability of a novel Mixture-of-Experts (MoE) architecture that uses low-rank compression to manage a large number of experts. The goal is to create a proof-of-concept to verify that the model can learn and that its validation perplexity decreases over time on a subset of the C4 dataset.\n",
    "\n",
    "### Core Architectural Ideas:\n",
    "1.  **DeepSeek-V3 Base**: The transformer blocks use **Multi-head Latent Attention (MLA)** and **RMSNorm**, inspired by the DeepSeek-V3 architecture.\n",
    "2.  **Low-Rank Expert Compression**: Instead of storing full parameters for millions of experts, the model stores a small **latent vector** for each expert. A shared **hypernetwork** (ExpertGenerationNetwork) generates the full expert weights on-the-fly from this latent vector.\n",
    "3.  **Product Key Routing with Multi-Head**: Implements the efficient routing mechanism from the PEER paper to select experts in sub-linear time, with multiple independent routing heads for diverse expert selection.\n",
    "4.  **Dense-then-Sparse Structure**: The first few layers of the model are dense FFNs to create robust representations, while the subsequent deeper layers are all MoE layers for specialized processing.\n",
    "5.  **Target Scale & Time**: The model is configured with the computational cost of a ~110M dense model, targeting a ~3-day training run on the C4 dataset on a single GH200. Its physical size is ~1.3B parameters, with a theoretical uncompressed capacity of ~8.7B parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install datasets tiktoken transformers tqdm deepspeed wandb matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "import tiktoken\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "class ModelConfig:\n",
    "    # Tiktoken cl100k_base vocab size\n",
    "    vocab_size = 100277\n",
    "    # Model dimensions for ~110M active params\n",
    "    d_model = 1024\n",
    "    num_layers = 16\n",
    "    num_heads = 16\n",
    "    # Dense-then-Sparse structure\n",
    "    num_dense_layers = 2\n",
    "    num_moe_layers = 14\n",
    "    # For MLA (Attention)\n",
    "    q_lora_rank = 768\n",
    "    kv_lora_rank = 256\n",
    "    v_head_dim = 64  # d_model / num_heads\n",
    "    qk_nope_head_dim = 64\n",
    "    qk_rope_head_dim = 64\n",
    "    # For Low-Rank MoE\n",
    "    num_experts = 512**2\n",
    "    d_latent = 128  # Reduced from 256\n",
    "    d_intermediate_hypernet = 512  # Reduced from 1024\n",
    "    top_k = 16\n",
    "    # Multi-head routing (PEER paper)\n",
    "    num_routing_heads = 8\n",
    "    # For Product Key Router\n",
    "    d_query = 512 # Dimension of the query vector for routing\n",
    "    # For FFN in dense layers\n",
    "    d_ffn_intermediate = 4096\n",
    "    # Other\n",
    "    max_seq_len = 4096\n",
    "    rms_norm_eps = 1e-6\n",
    "    rope_theta = 10000.0\n",
    "\n",
    "config = ModelConfig()\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Logging and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging directory\n",
    "LOG_DIR = \"./training_logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Generate unique run ID\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Run ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"Combined logger and visualizer for training metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, run_id, log_dir=\"./training_logs\", plot_window=500):\n",
    "        self.run_id = run_id\n",
    "        self.log_dir = log_dir\n",
    "        self.plot_window = plot_window\n",
    "        \n",
    "        # Setup logging files\n",
    "        self.json_log_path = os.path.join(log_dir, f\"training_{run_id}.jsonl\")\n",
    "        self.csv_log_path = os.path.join(log_dir, f\"metrics_{run_id}.csv\")\n",
    "        \n",
    "        # Initialize CSV with headers\n",
    "        with open(self.csv_log_path, 'w') as f:\n",
    "            f.write(\"timestamp,epoch,batch,global_step,loss,perplexity,lr,gpu_memory_mb,avg_loss_100\\n\")\n",
    "        \n",
    "        # Initialize metrics storage for plotting\n",
    "        self.metrics = {\n",
    "            'steps': deque(maxlen=plot_window),\n",
    "            'loss': deque(maxlen=plot_window),\n",
    "            'perplexity': deque(maxlen=plot_window),\n",
    "            'lr': deque(maxlen=plot_window),\n",
    "            'gpu_memory': deque(maxlen=plot_window),\n",
    "            'avg_loss': deque(maxlen=plot_window)\n",
    "        }\n",
    "        \n",
    "        # For computing running averages\n",
    "        self.recent_losses = deque(maxlen=100)\n",
    "        \n",
    "        # Setup matplotlib for notebook\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.plot_initialized = False\n",
    "        \n",
    "        # Validation metrics\n",
    "        self.val_history = {\n",
    "            'epoch': [],\n",
    "            'loss': [],\n",
    "            'perplexity': []\n",
    "        }\n",
    "        \n",
    "        print(f\"Logging to: {self.json_log_path}\")\n",
    "        print(f\"Metrics CSV: {self.csv_log_path}\")\n",
    "    \n",
    "    def log_iteration(self, epoch, batch, global_step, loss, lr, \n",
    "                     batch_size=None, seq_len=None, extra_metrics=None):\n",
    "        \"\"\"Log a training iteration\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        perplexity = np.exp(loss) if loss < 50 else float('inf')  # Avoid overflow\n",
    "        gpu_memory_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        \n",
    "        # Update running average\n",
    "        self.recent_losses.append(loss)\n",
    "        avg_loss = np.mean(self.recent_losses)\n",
    "        \n",
    "        # Create log entry\n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'run_id': self.run_id,\n",
    "            'epoch': epoch,\n",
    "            'batch': batch,\n",
    "            'global_step': global_step,\n",
    "            'loss': float(loss),\n",
    "            'perplexity': float(perplexity),\n",
    "            'lr': float(lr),\n",
    "            'gpu_memory_mb': float(gpu_memory_mb),\n",
    "            'avg_loss_100': float(avg_loss)\n",
    "        }\n",
    "        \n",
    "        # Add optional metrics\n",
    "        if batch_size:\n",
    "            log_entry['batch_size'] = batch_size\n",
    "        if seq_len:\n",
    "            log_entry['seq_len'] = seq_len\n",
    "        if extra_metrics:\n",
    "            log_entry.update(extra_metrics)\n",
    "        \n",
    "        # Write to JSON log\n",
    "        with open(self.json_log_path, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "        \n",
    "        # Write to CSV\n",
    "        csv_row = f\"{timestamp},{epoch},{batch},{global_step},{loss:.6f},\" \\\n",
    "                  f\"{perplexity:.2f},{lr:.8f},{gpu_memory_mb:.1f},{avg_loss:.6f}\\n\"\n",
    "        with open(self.csv_log_path, 'a') as f:\n",
    "            f.write(csv_row)\n",
    "        \n",
    "        # Update metrics for plotting\n",
    "        self.metrics['steps'].append(global_step)\n",
    "        self.metrics['loss'].append(loss)\n",
    "        self.metrics['perplexity'].append(perplexity)\n",
    "        self.metrics['lr'].append(lr)\n",
    "        self.metrics['gpu_memory'].append(gpu_memory_mb)\n",
    "        self.metrics['avg_loss'].append(avg_loss)\n",
    "        \n",
    "        return log_entry\n",
    "    \n",
    "    def log_validation(self, epoch, val_loss, val_perplexity):\n",
    "        \"\"\"Log validation metrics\"\"\"\n",
    "        self.val_history['epoch'].append(epoch)\n",
    "        self.val_history['loss'].append(val_loss)\n",
    "        self.val_history['perplexity'].append(val_perplexity)\n",
    "        \n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'run_id': self.run_id,\n",
    "            'type': 'validation',\n",
    "            'epoch': epoch,\n",
    "            'val_loss': float(val_loss),\n",
    "            'val_perplexity': float(val_perplexity)\n",
    "        }\n",
    "        \n",
    "        with open(self.json_log_path, 'a') as f:\n",
    "            f.write(json.dumps(log_entry) + '\\n')\n",
    "    \n",
    "    def plot_metrics(self, update_frequency=50):\n",
    "        \"\"\"Update real-time plots in notebook\"\"\"\n",
    "        if len(self.metrics['steps']) < 2:\n",
    "            return\n",
    "        \n",
    "        current_step = self.metrics['steps'][-1]\n",
    "        \n",
    "        # Only update plot every N steps to reduce overhead\n",
    "        if current_step % update_frequency != 0 and current_step > update_frequency:\n",
    "            return\n",
    "        \n",
    "        if not self.plot_initialized:\n",
    "            # plt.style.use('seaborn-v0_8-darkgrid')\n",
    "            self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            self.fig.suptitle(f'Training Metrics - Run {self.run_id}', fontsize=16)\n",
    "            self.plot_initialized = True\n",
    "        \n",
    "        # Clear previous plots\n",
    "        for ax in self.axes.flat:\n",
    "            ax.clear()\n",
    "        \n",
    "        # Convert deques to lists for plotting\n",
    "        steps = list(self.metrics['steps'])\n",
    "        \n",
    "        # Plot 1: Loss\n",
    "        ax = self.axes[0, 0]\n",
    "        ax.plot(steps, list(self.metrics['loss']), 'b-', alpha=0.3, label='Raw Loss')\n",
    "        ax.plot(steps, list(self.metrics['avg_loss']), 'r-', linewidth=2, label='Avg Loss (100)')\n",
    "        ax.set_xlabel('Global Step')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training Loss')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(bottom=0)\n",
    "        \n",
    "        # Plot 2: Perplexity\n",
    "        ax = self.axes[0, 1]\n",
    "        perplexity_values = [p for p in self.metrics['perplexity'] if p < 1000]  # Filter outliers\n",
    "        if perplexity_values:\n",
    "            ax.plot(steps[-len(perplexity_values):], perplexity_values, 'g-')\n",
    "            ax.set_xlabel('Global Step')\n",
    "            ax.set_ylabel('Perplexity')\n",
    "            ax.set_title('Perplexity')\n",
    "            ax.set_ylim(bottom=0)\n",
    "        \n",
    "        # Plot 3: Learning Rate\n",
    "        ax = self.axes[1, 0]\n",
    "        ax.plot(steps, list(self.metrics['lr']), 'orange')\n",
    "        ax.set_xlabel('Global Step')\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.set_title('Learning Rate Schedule')\n",
    "        ax.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "        \n",
    "        # Plot 4: GPU Memory\n",
    "        ax = self.axes[1, 1]\n",
    "        ax.plot(steps, list(self.metrics['gpu_memory']), 'purple')\n",
    "        ax.set_xlabel('Global Step')\n",
    "        ax.set_ylabel('GPU Memory (MB)')\n",
    "        ax.set_title('GPU Memory Usage')\n",
    "        ax.axhline(y=52000, color='r', linestyle='--', alpha=0.5, label='GPU Max')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Add validation points if available\n",
    "        if self.val_history['epoch']:\n",
    "            # Find the step numbers for validation epochs\n",
    "            # This is approximate - you might want to track exact validation steps\n",
    "            val_steps = [epoch * (steps[-1] // max(1, self.val_history['epoch'][-1])) \n",
    "                        for epoch in self.val_history['epoch']]\n",
    "            \n",
    "            # Add validation markers to loss plot\n",
    "            self.axes[0, 0].scatter(val_steps, self.val_history['loss'], \n",
    "                                  color='red', s=100, marker='*', \n",
    "                                  label='Validation', zorder=5)\n",
    "            self.axes[0, 0].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Display in notebook\n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "        plt.pause(0.01)\n",
    "    \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"Get summary statistics for the current run\"\"\"\n",
    "        if not self.metrics['loss']:\n",
    "            return {}\n",
    "        \n",
    "        recent_losses = list(self.metrics['loss'])[-100:]\n",
    "        \n",
    "        return {\n",
    "            'total_steps': len(self.metrics['steps']),\n",
    "            'current_loss': self.metrics['loss'][-1],\n",
    "            'avg_recent_loss': np.mean(recent_losses),\n",
    "            'min_loss': min(self.metrics['loss']),\n",
    "            'current_lr': self.metrics['lr'][-1],\n",
    "            'current_gpu_mb': self.metrics['gpu_memory'][-1],\n",
    "            'best_val_perplexity': min(self.val_history['perplexity']) if self.val_history['perplexity'] else None\n",
    "        }\n",
    "\n",
    "# Initialize the monitor\n",
    "monitor = TrainingMonitor(RUN_ID)\n",
    "print(\"Training monitor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Implementation\n",
    "\n",
    "We define the PyTorch modules for our experiment, now including the multi-head `ProductKeyRouter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Custom fused kernel with reordered execution\n",
    "\n",
    "This kernel implements a more efficient execution order:\n",
    "1. Project tokens to hidden dimension once (not per expert)\n",
    "2. Compute expert activations in the smaller hidden dimension\n",
    "3. Project back to token dimension only at the end\n",
    "\n",
    "This reduces memory usage ~4x and improves speed ~8x based on benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def moe_reorder_kernel(\n",
    "    # Inputs\n",
    "    x_ptr,               # [B*S, d_model]\n",
    "    latent_ptr,          # [num_experts, d_latent]\n",
    "    indices_ptr,         # [B*S, top_k]\n",
    "    scores_ptr,          # [B*S, top_k]\n",
    "    \n",
    "    wu_ptr,              # [d_model, d_hidden] - W_u for token→hidden projection\n",
    "    w1_ptr,              # [d_latent, d_hidden]\n",
    "    wv_ptr,              # [d_hidden, d_model] - W_v for hidden→token projection\n",
    "    \n",
    "    out_ptr,             # [B*S, d_model]\n",
    "    \n",
    "    # Dimensions\n",
    "    batch_seq_size, d_model, d_latent, d_hidden, top_k,\n",
    "    \n",
    "    # Strides\n",
    "    stride_x_bs, stride_x_d,\n",
    "    stride_idx_bs, stride_idx_k,\n",
    "    stride_score_bs, stride_score_k,\n",
    "    stride_out_bs, stride_out_d,\n",
    "    stride_latent_n, stride_latent_d,\n",
    "    \n",
    "    stride_wu_row, stride_wu_col,\n",
    "    stride_w1_row, stride_w1_col,\n",
    "    stride_wv_row, stride_wv_col,\n",
    "    \n",
    "    # Block sizes (compile-time constants)\n",
    "    BLOCK_DMODEL: tl.constexpr,\n",
    "    BLOCK_DHIDDEN: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    if pid >= batch_seq_size:\n",
    "        return\n",
    "    \n",
    "    # ------------------------------------------------------------------ \n",
    "    # Step 1: Project token from d_model → d_hidden using W_u\n",
    "    # This is done once per token, not per expert\n",
    "    # ------------------------------------------------------------------\n",
    "    d_offs = tl.arange(0, BLOCK_DMODEL)          # indices for d_model dimension\n",
    "    h_offs = tl.arange(0, BLOCK_DHIDDEN)         # indices for d_hidden dimension\n",
    "    h_mask = h_offs < d_hidden\n",
    "    \n",
    "    x_proj = tl.zeros([BLOCK_DHIDDEN], dtype=tl.float32)\n",
    "    \n",
    "    # Compute x_proj = x @ W_u\n",
    "    # Process in blocks if d_model > BLOCK_DMODEL\n",
    "    for d_start in range(0, d_model, BLOCK_DMODEL):\n",
    "        d_chunk = d_start + d_offs\n",
    "        x_mask = d_chunk < d_model\n",
    "        \n",
    "        # Load chunk of input token\n",
    "        x_chunk = tl.load(\n",
    "            x_ptr + pid * stride_x_bs + d_chunk * stride_x_d,\n",
    "            mask=x_mask,\n",
    "            other=0.0,\n",
    "        )\n",
    "        \n",
    "        # Load corresponding chunk of W_u: [d_chunk, h_offs]\n",
    "        w_chunk = tl.load(\n",
    "            wu_ptr + d_chunk[:, None] * stride_wu_row + h_offs[None, :] * stride_wu_col,\n",
    "            mask=x_mask[:, None] & h_mask[None, :],\n",
    "            other=0.0,\n",
    "        )\n",
    "        \n",
    "        # Accumulate matrix multiplication\n",
    "        x_proj += tl.sum(x_chunk[:, None] * w_chunk, axis=0)\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Step 2: Process each expert\n",
    "    # ------------------------------------------------------------------\n",
    "    output = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n",
    "    \n",
    "    for k in range(top_k):\n",
    "        expert_idx = tl.load(indices_ptr + pid * stride_idx_bs + k * stride_idx_k)\n",
    "        score = tl.load(scores_ptr + pid * stride_score_bs + k * stride_score_k)\n",
    "        \n",
    "        # Compute recovery activations: a^r = GELU(latent @ W1)\n",
    "        h = tl.zeros([BLOCK_DHIDDEN], dtype=tl.float32)\n",
    "        for l in range(d_latent):\n",
    "            latent_val = tl.load(latent_ptr + expert_idx * stride_latent_n + l * stride_latent_d)\n",
    "            w1_row = tl.load(\n",
    "                w1_ptr + l * stride_w1_row + h_offs * stride_w1_col,\n",
    "                mask=h_mask,\n",
    "                other=0.0,\n",
    "            )\n",
    "            h += latent_val * w1_row\n",
    "        \n",
    "        # Apply GELU activation\n",
    "        h = h * tl.sigmoid(1.702 * h)  # GELU approximation\n",
    "        \n",
    "        # Compute scalar activation: a^e = GELU(h · x_proj) * score\n",
    "        dot = tl.sum(h * x_proj)\n",
    "        activation = dot * tl.sigmoid(1.702 * dot) * score\n",
    "        \n",
    "        # Project back to token space: output += activation * (h @ W_v)\n",
    "        for d_start in range(0, d_model, BLOCK_DMODEL):\n",
    "            d_chunk = d_start + d_offs\n",
    "            x_mask = d_chunk < d_model\n",
    "            \n",
    "            # Load W_v block: [h_offs, d_chunk]\n",
    "            wv_block = tl.load(\n",
    "                wv_ptr + h_offs[:, None] * stride_wv_row + d_chunk[None, :] * stride_wv_col,\n",
    "                mask=h_mask[:, None] & x_mask[None, :],\n",
    "                other=0.0,\n",
    "            )\n",
    "            \n",
    "            # Compute contribution to output\n",
    "            proj = tl.sum(h[:, None] * wv_block, axis=0)\n",
    "            \n",
    "            # Store accumulated result (using atomic add for safety)\n",
    "            out_ptr_offset = pid * stride_out_bs + d_chunk * stride_out_d\n",
    "            old = tl.load(out_ptr + out_ptr_offset, mask=x_mask, other=0.0)\n",
    "            tl.store(out_ptr + out_ptr_offset, old + activation * proj, mask=x_mask)\n",
    "\n",
    "\n",
    "class FusedLowRankMoE_Reordered(nn.Module):\n",
    "    \"\"\"\n",
    "    Reordered Low-Rank MoE that uses the more efficient execution pattern:\n",
    "    1. Project tokens to hidden dimension once\n",
    "    2. Compute expert activations in hidden dimension\n",
    "    3. Project back to token dimension at the end\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.num_experts = config.num_experts\n",
    "        self.top_k = config.top_k\n",
    "        self.num_heads = config.num_routing_heads\n",
    "        self.d_latent = config.d_latent\n",
    "        self.d_hidden = config.d_intermediate_hypernet\n",
    "        \n",
    "        self.expert_latents = nn.Embedding(config.num_experts, config.d_latent)\n",
    "        self.generation_network = ExpertGenerationNetwork(\n",
    "            config.d_latent, config.d_model, config.d_intermediate_hypernet\n",
    "        )\n",
    "        self.router = ProductKeyRouter(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flat = x.view(-1, self.d_model)\n",
    "        n_tokens = x_flat.shape[0]\n",
    "        \n",
    "        # Get routing scores and indices for all heads\n",
    "        scores, expert_indices = self.router(x_flat)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # Get shared weight matrices\n",
    "        W1 = self.generation_network.net[0].weight.t().contiguous()  # [d_latent, d_hidden]\n",
    "        W2 = self.generation_network.net[2].weight.t().contiguous()  # [d_hidden, 2*d_model]\n",
    "        \n",
    "        # Split W2 into W_u and W_v\n",
    "        W_u = W2[:, :self.d_model].t().contiguous()     # [d_model, d_hidden]\n",
    "        W_v = W2[:, self.d_model:].contiguous()         # [d_hidden, d_model]\n",
    "        \n",
    "        expert_latents_contig = self.expert_latents.weight.contiguous()\n",
    "        \n",
    "        # Process each routing head\n",
    "        for head_idx in range(self.num_heads):\n",
    "            head_scores = scores[:, head_idx, :].contiguous()\n",
    "            head_indices = expert_indices[:, head_idx, :].contiguous()\n",
    "            head_output = torch.zeros_like(x_flat)\n",
    "            \n",
    "            # Block sizes\n",
    "            BLOCK_DMODEL = min(1024, self.d_model)\n",
    "            BLOCK_DHIDDEN = min(64, self.d_hidden)\n",
    "            \n",
    "            # Launch kernel\n",
    "            grid = (n_tokens,)\n",
    "            \n",
    "            moe_reorder_kernel[grid](\n",
    "                # Inputs\n",
    "                x_flat,\n",
    "                expert_latents_contig,\n",
    "                head_indices,\n",
    "                head_scores,\n",
    "                \n",
    "                # Weight matrices\n",
    "                W_u,\n",
    "                W1,\n",
    "                W_v,\n",
    "                \n",
    "                # Output\n",
    "                head_output,\n",
    "                \n",
    "                # Dimensions\n",
    "                n_tokens,\n",
    "                self.d_model,\n",
    "                self.d_latent,\n",
    "                self.d_hidden,\n",
    "                self.top_k,\n",
    "                \n",
    "                # Strides\n",
    "                x_flat.stride(0), x_flat.stride(1),\n",
    "                head_indices.stride(0), head_indices.stride(1),\n",
    "                head_scores.stride(0), head_scores.stride(1),\n",
    "                head_output.stride(0), head_output.stride(1),\n",
    "                expert_latents_contig.stride(0), expert_latents_contig.stride(1),\n",
    "                \n",
    "                W_u.stride(0), W_u.stride(1),\n",
    "                W1.stride(0), W1.stride(1),\n",
    "                W_v.stride(0), W_v.stride(1),\n",
    "                \n",
    "                # Block sizes\n",
    "                BLOCK_DMODEL=BLOCK_DMODEL,\n",
    "                BLOCK_DHIDDEN=BLOCK_DHIDDEN,\n",
    "            )\n",
    "            \n",
    "            output += head_output\n",
    "        \n",
    "        # Average across heads\n",
    "        output = output / self.num_heads\n",
    "        return output.view(batch_size, seq_len, self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model Architecture\n",
    "\n",
    "Our model implements:\n",
    "1. Product Key routing to experts, to allow for efficient scaling past 1e6 experts.\n",
    "2. MLA in line with DeepSeek-V3's implementation\n",
    "3. Low Rank experts at initialization, that are expanded on the fly by our expert generation hypernetwork (unique per layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from DeepSeek-V3 modeling code\n",
    "class DeepseekV3RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "class DeepseekV3RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device, dtype=torch.get_default_dtype())\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = emb.cos().to(dtype)\n",
    "        self.sin_cached = emb.sin().to(dtype)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        if self.cos_cached is None or seq_len > self.cos_cached.shape[0]:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # Handle case where position_ids is 1D [seq_len]\n",
    "    if position_ids.dim() == 1:\n",
    "        position_ids = position_ids.unsqueeze(0)  # [1, seq_len]\n",
    "    \n",
    "    cos = cos[position_ids].unsqueeze(1)\n",
    "    sin = sin[position_ids].unsqueeze(1)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "class DeepseekV3Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.d_model\n",
    "        self.num_heads = config.num_heads\n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "\n",
    "        self.q_a_proj = nn.Linear(self.hidden_size, config.q_lora_rank, bias=False)\n",
    "        self.q_a_layernorm = DeepseekV3RMSNorm(config.q_lora_rank, eps=config.rms_norm_eps)\n",
    "        self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.q_head_dim, bias=False)\n",
    "\n",
    "        self.kv_a_proj_with_mqa = nn.Linear(self.hidden_size, config.kv_lora_rank + config.qk_rope_head_dim, bias=False)\n",
    "        self.kv_a_layernorm = DeepseekV3RMSNorm(config.kv_lora_rank, eps=config.rms_norm_eps)\n",
    "        self.kv_b_proj = nn.Linear(config.kv_lora_rank, self.num_heads * (self.qk_nope_head_dim + self.v_head_dim), bias=False)\n",
    "\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.v_head_dim, self.hidden_size, bias=False)\n",
    "        self.rotary_emb = DeepseekV3RotaryEmbedding(self.qk_rope_head_dim, max_position_embeddings=config.max_seq_len, base=config.rope_theta, device=device)\n",
    "        \n",
    "        # Add these for flash attention\n",
    "        self.is_causal = True\n",
    "        self.attention_dropout = 0.0  # or from config\n",
    "        self.softmax_scale = self.q_head_dim ** (-0.5)\n",
    "        self._flash_attn_uses_top_left_mask = False  # For flash_attn >= 2.1\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, position_ids):\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n",
    "        q = q.view(bsz, q_len, self.num_heads, self.q_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "\n",
    "        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n",
    "        compressed_kv, k_pe = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        k_pe = k_pe.view(bsz, q_len, 1, self.qk_rope_head_dim)\n",
    "        kv = self.kv_b_proj(self.kv_a_layernorm(compressed_kv)).view(bsz, q_len, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "\n",
    "        k_nope, value_states = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
    "        q_pe, k_pe = apply_rotary_pos_emb(q_pe.transpose(1, 2), k_pe.transpose(1, 2), cos, sin, position_ids)\n",
    "\n",
    "        # Combine nope and pe parts\n",
    "        query_states = torch.cat([q_nope.transpose(1, 2), q_pe], dim=-1)\n",
    "        key_states = torch.cat([k_nope.transpose(1, 2), k_pe.expand(bsz, self.num_heads, q_len, self.qk_rope_head_dim)], dim=-1)\n",
    "        value_states = value_states.transpose(1, 2)\n",
    "\n",
    "        # Use Flash Attention\n",
    "        from flash_attn import flash_attn_func\n",
    "        \n",
    "        # Flash attention expects (batch, seqlen, nheads, headdim)\n",
    "        query_states = query_states.transpose(1, 2).contiguous()\n",
    "        key_states = key_states.transpose(1, 2).contiguous()\n",
    "        value_states = value_states.transpose(1, 2).contiguous()\n",
    "        \n",
    "        # Pad value states if needed\n",
    "        if self.q_head_dim != self.v_head_dim:\n",
    "            value_states = F.pad(value_states, [0, self.q_head_dim - self.v_head_dim])\n",
    "        \n",
    "        attn_output = flash_attn_func(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            softmax_scale=self.softmax_scale,\n",
    "            causal=self.is_causal\n",
    "        )\n",
    "        \n",
    "        if self.q_head_dim != self.v_head_dim:\n",
    "            attn_output = attn_output[:, :, :, :self.v_head_dim]\n",
    "        \n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.v_head_dim)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class ExpertGenerationNetwork(nn.Module):\n",
    "    def __init__(self, d_latent, d_model, d_intermediate):\n",
    "        super().__init__()\n",
    "        self.d_expert_params = 2 * d_model\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_latent, d_intermediate, bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_intermediate, self.d_expert_params, bias=False)\n",
    "        )\n",
    "    def forward(self, latent_vector):\n",
    "        return self.net(latent_vector)\n",
    "\n",
    "class ProductKeyRouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.num_experts = config.num_experts\n",
    "        self.top_k = config.top_k\n",
    "        self.d_query = config.d_query\n",
    "        self.num_heads = config.num_routing_heads\n",
    "\n",
    "        self.num_sub_keys = int(math.sqrt(self.num_experts))\n",
    "        assert self.num_sub_keys**2 == self.num_experts, \"num_experts must be a perfect square\"\n",
    "\n",
    "        # Multi-head query projections\n",
    "        self.query_projs = nn.ModuleList([\n",
    "            nn.Linear(self.d_model, self.d_query) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization per head (as per PEER paper)\n",
    "        self.query_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(self.d_query) for _ in range(self.num_heads)\n",
    "        ])\n",
    "        \n",
    "        # Shared sub-keys across heads\n",
    "        self.sub_keys_1 = nn.Embedding(self.num_sub_keys, self.d_query // 2)\n",
    "        self.sub_keys_2 = nn.Embedding(self.num_sub_keys, self.d_query // 2)\n",
    "\n",
    "    def forward(self, x_flat):\n",
    "        # x_flat shape: (batch*seq, d_model)\n",
    "        batch_seq_len = x_flat.shape[0]\n",
    "        \n",
    "        all_scores = []\n",
    "        all_indices = []\n",
    "        \n",
    "        for head_idx in range(self.num_heads):\n",
    "            query = self.query_projs[head_idx](x_flat)\n",
    "            query = self.query_norms[head_idx](query)\n",
    "            q1, q2 = query.chunk(2, dim=-1)\n",
    "\n",
    "            # Calculate scores against each sub-key table\n",
    "            scores1 = q1 @ self.sub_keys_1.weight.t()\n",
    "            scores2 = q2 @ self.sub_keys_2.weight.t()\n",
    "\n",
    "            # Find top candidates from each sub-key set\n",
    "            k_cand = self.top_k * 2  # Heuristic: check more candidates\n",
    "            top_scores1, top_indices1 = torch.topk(scores1, k_cand, dim=-1)\n",
    "            top_scores2, top_indices2 = torch.topk(scores2, k_cand, dim=-1)\n",
    "\n",
    "            # Combine scores of candidate pairs\n",
    "            combined_scores = top_scores1.unsqueeze(2) + top_scores2.unsqueeze(1)\n",
    "            combined_scores = combined_scores.view(batch_seq_len, -1)\n",
    "\n",
    "            # Find the final top_k from the candidate pairs\n",
    "            final_scores, top_combined_indices = torch.topk(combined_scores, self.top_k, dim=-1)\n",
    "            final_scores = F.softmax(final_scores, dim=-1, dtype=torch.float).to(x_flat.dtype)\n",
    "\n",
    "            # Decode the combined indices to get the original sub-key indices\n",
    "            idx_from_1 = top_combined_indices // k_cand\n",
    "            idx_from_2 = top_combined_indices % k_cand\n",
    "\n",
    "            # Gather the final sub-key indices\n",
    "            final_indices_1 = top_indices1.gather(1, idx_from_1)\n",
    "            final_indices_2 = top_indices2.gather(1, idx_from_2)\n",
    "\n",
    "            # Calculate the final 1D expert index\n",
    "            final_expert_indices = final_indices_1 * self.num_sub_keys + final_indices_2\n",
    "            \n",
    "            all_scores.append(final_scores)\n",
    "            all_indices.append(final_expert_indices)\n",
    "        \n",
    "        # Stack results: [batch*seq, num_heads, top_k]\n",
    "        all_scores = torch.stack(all_scores, dim=1)\n",
    "        all_indices = torch.stack(all_indices, dim=1)\n",
    "        \n",
    "        return all_scores, all_indices\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.d_model, config.d_ffn_intermediate, bias=False)\n",
    "        self.w2 = nn.Linear(config.d_ffn_intermediate, config.d_model, bias=False)\n",
    "        self.w3 = nn.Linear(config.d_model, config.d_ffn_intermediate, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config, is_moe_layer):\n",
    "        super().__init__()\n",
    "        self.self_attn = DeepseekV3Attention(config)\n",
    "        # Use the reordered MoE implementation\n",
    "        self.mlp = FusedLowRankMoE_Reordered(config) if is_moe_layer else FFN(config)\n",
    "        self.input_layernorm = DeepseekV3RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = DeepseekV3RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask, position_ids):\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask, position_ids)\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class CompressedMoEModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tok_embeddings = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(config.num_layers):\n",
    "            is_moe_layer = i >= config.num_dense_layers\n",
    "            layers.append(TransformerBlock(config, is_moe_layer))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        self.norm = DeepseekV3RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.tok_embeddings.weight = self.output.weight\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        h = self.tok_embeddings(input_ids)\n",
    "\n",
    "        attention_mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=input_ids.device, dtype=h.dtype)\n",
    "        attention_mask = torch.triu(attention_mask, diagonal=1)\n",
    "        \n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, attention_mask, position_ids)\n",
    "\n",
    "        return self.output(self.norm(h))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preparation\n",
    "\n",
    "We will load only 1% of the C4 dataset using streaming mode to avoid downloading the entire dataset. This significantly speeds up the data loading process for testing.\n",
    "\n",
    "**NOTE**: The percentage can be adjusted by changing the `DATA_PERCENTAGE` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "DATA_PERCENTAGE = 0.01\n",
    "print(f\"Loading {DATA_PERCENTAGE*100:.1f}% of C4 dataset...\")\n",
    "import os\n",
    "CACHE_DIR = \"./c4_tokenized_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "train_cache_path = os.path.join(CACHE_DIR, f\"train_{DATA_PERCENTAGE}\")\n",
    "val_cache_path = os.path.join(CACHE_DIR, f\"val_{DATA_PERCENTAGE}\")\n",
    "\n",
    "if os.path.exists(train_cache_path) and os.path.exists(val_cache_path):\n",
    "    print(\"Loading tokenized datasets from cache...\")\n",
    "    train_tokenized = Dataset.load_from_disk(train_cache_path)\n",
    "    val_tokenized = Dataset.load_from_disk(val_cache_path)\n",
    "    print(f\"Loaded from cache - Train: {len(train_tokenized):,}, Val: {len(val_tokenized):,}\")\n",
    "else:\n",
    "    print(\"Downloading C4 dataset...\")\n",
    "    if DATA_PERCENTAGE < 1.0:\n",
    "        split_percentage = int(DATA_PERCENTAGE * 100)\n",
    "        train_dataset = load_dataset(\"allenai/c4\", \"en\", split=f\"train[:{split_percentage}%]\")\n",
    "        val_dataset = load_dataset(\"allenai/c4\", \"en\", split=f\"validation[:{split_percentage}%]\")\n",
    "    else:\n",
    "        train_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\")\n",
    "        val_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"validation\")\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset):,}, Val samples: {len(val_dataset):,}\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        token_ids = enc.encode(examples[\"text\"], allowed_special=\"all\")\n",
    "        token_ids = token_ids[:config.max_seq_len]\n",
    "        return {\"input_ids\": token_ids}\n",
    "    \n",
    "    print(\"Tokenizing datasets...\")\n",
    "    train_tokenized = train_dataset.map(\n",
    "        tokenize_function, \n",
    "        remove_columns=[\"text\", \"timestamp\", \"url\"],\n",
    "        num_proc=64,\n",
    "        desc=\"Tokenizing train set\"\n",
    "    )\n",
    "    val_tokenized = val_dataset.map(\n",
    "        tokenize_function, \n",
    "        remove_columns=[\"text\", \"timestamp\", \"url\"],\n",
    "        num_proc=64,\n",
    "        desc=\"Tokenizing validation set\"\n",
    "    )\n",
    "    \n",
    "    print(\"Saving tokenized datasets to cache...\")\n",
    "    train_tokenized.save_to_disk(train_cache_path)\n",
    "    val_tokenized.save_to_disk(val_cache_path)\n",
    "    print(\"Cache saved successfully!\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=enc.eot_token)\n",
    "    return {'input_ids': padded_inputs}\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Use num_workers=0 to avoid multiprocessing issues\n",
    "train_loader = DataLoader(\n",
    "    train_tokenized, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_batch, \n",
    "    num_workers=0,  # Single process\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_batch, \n",
    "    num_workers=0,  # Single process\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created with single-process loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization and Training\n",
    "\n",
    "We initialize the model, calculate its parameter count, and set up the training loop. For multi-GPU training, we'll use either DataParallel or FSDP depending on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepspeed\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Initialize distributed environment for single GPU\n",
    "if not dist.is_initialized():\n",
    "    os.environ['RANK'] = '0'\n",
    "    os.environ['WORLD_SIZE'] = '1'\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    os.environ['LOCAL_RANK'] = '0'\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    # Initialize the process group\n",
    "    dist.init_process_group(backend='nccl', rank=0, world_size=1)\n",
    "\n",
    "# DeepSpeed configuration with more aggressive memory optimization\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 1e-4,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 1e-5,\n",
    "            \"warmup_max_lr\": 1e-4,\n",
    "            \"warmup_num_steps\": 1000\n",
    "        }\n",
    "    },\n",
    "    \"bf16\": {\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        # \"offload_optimizer\": {\n",
    "        #     \"device\": \"cpu\",\n",
    "        #     \"pin_memory\": True\n",
    "        # },\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"sub_group_size\": 1e9,\n",
    "        \"reduce_bucket_size\": 1e8,\n",
    "        # \"stage3_prefetch_bucket_size\": 1e7,\n",
    "        # \"stage3_param_persistence_threshold\": 1e5,\n",
    "        # \"stage3_max_live_parameters\": 1e8,\n",
    "        # \"stage3_max_reuse_distance\": 1e8\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"steps_per_print\": 100,\n",
    "    \"wall_clock_breakdown\": False,\n",
    "    # \"activation_checkpointing\": {\n",
    "    #     \"partition_activations\": True,\n",
    "    #     \"cpu_checkpointing\": True,\n",
    "    #     \"contiguous_memory_optimization\": False,\n",
    "    #     \"number_checkpoints\": None,\n",
    "    #     \"synchronize_checkpoint_boundary\": False,\n",
    "    #     \"profile\": False\n",
    "    # }\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = CompressedMoEModel(config)\n",
    "\n",
    "# model = torch.compile(\n",
    "#     model, \n",
    "#     mode='reduce-overhead',  # Better for training\n",
    "#     fullgraph=False,  # Allow graph breaks for dynamic routing\n",
    "#     dynamic=True  # Handle variable sequence lengths\n",
    "# )\n",
    "\n",
    "# Calculate parameter counts\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params / 1e9:.2f}B\")\n",
    "\n",
    "# Calculate active parameters per forward pass\n",
    "dense_active = config.num_dense_layers * (\n",
    "    config.d_model * config.q_lora_rank + config.q_lora_rank * config.num_heads * (config.qk_nope_head_dim + config.qk_rope_head_dim) +\n",
    "    config.d_model * (config.kv_lora_rank + config.qk_rope_head_dim) + config.kv_lora_rank * config.num_heads * (config.qk_nope_head_dim + config.v_head_dim) +\n",
    "    config.num_heads * config.v_head_dim * config.d_model +\n",
    "    3 * config.d_model * config.d_ffn_intermediate\n",
    ")\n",
    "\n",
    "moe_active = config.num_moe_layers * (\n",
    "    config.d_model * config.q_lora_rank + config.q_lora_rank * config.num_heads * (config.qk_nope_head_dim + config.qk_rope_head_dim) +\n",
    "    config.d_model * (config.kv_lora_rank + config.qk_rope_head_dim) + config.kv_lora_rank * config.num_heads * (config.qk_nope_head_dim + config.v_head_dim) +\n",
    "    config.num_heads * config.v_head_dim * config.d_model +\n",
    "    config.num_routing_heads * config.d_model * config.d_query +\n",
    "    config.num_routing_heads * config.top_k * 2 * config.d_model\n",
    ")\n",
    "\n",
    "embedding_params = config.vocab_size * config.d_model\n",
    "total_active = dense_active + moe_active + embedding_params\n",
    "print(f\"Active Parameters per Token: ~{total_active / 1e6:.1f}M\")\n",
    "\n",
    "# Initialize DeepSpeed\n",
    "model_engine, optimizer, _, scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=ds_config\n",
    ")\n",
    "\n",
    "print(\"Running on single GPU with ZeRO-3 optimization\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(\n",
    "    train_tokenized, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_batch, \n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_tokenized, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate_batch, \n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=enc.eot_token)\n",
    "\n",
    "# Calculate actual number of batches\n",
    "num_train_batches = len(train_tokenized) // BATCH_SIZE\n",
    "num_val_batches = len(val_tokenized) // BATCH_SIZE\n",
    "print(f\"Training batches per epoch: {num_train_batches:,}\")\n",
    "print(f\"Validation batches per epoch: {num_val_batches:,}\")\n",
    "\n",
    "# If DeepSpeed is consuming 2 DataLoader batches per step, adjust the progress bar\n",
    "# Check if DeepSpeed is accumulating DataLoader batches\n",
    "deepspeed_accumulation = ds_config[\"train_micro_batch_size_per_gpu\"] // BATCH_SIZE if ds_config[\"train_micro_batch_size_per_gpu\"] > BATCH_SIZE else 1\n",
    "actual_steps = num_train_batches // deepspeed_accumulation\n",
    "\n",
    "print(f\"DeepSpeed micro batch size: {ds_config['train_micro_batch_size_per_gpu']}\")\n",
    "print(f\"DataLoader batch size: {BATCH_SIZE}\")\n",
    "print(f\"Actual training steps per epoch: {actual_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop with Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 3\n",
    "PLOT_UPDATE_FREQUENCY = 50  # Update plots every N steps\n",
    "LOG_EVERY_N_STEPS = 1  # Log every step\n",
    "\n",
    "# Modified training loop with monitoring\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_engine.train()\n",
    "    \n",
    "    # Calculate steps for progress bar\n",
    "    train_progress_bar = tqdm(\n",
    "        train_loader, \n",
    "        desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\",\n",
    "        total=actual_steps if deepspeed_accumulation > 1 else num_train_batches\n",
    "    )\n",
    "    \n",
    "    epoch_start_time = datetime.now()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_progress_bar):\n",
    "        # Get batch data\n",
    "        inputs = batch['input_ids'].to(model_engine.device)\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        labels = inputs.clone()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model_engine(inputs)\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        model_engine.backward(loss)\n",
    "        model_engine.step()\n",
    "        \n",
    "        # Get current metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        global_step = model_engine.global_steps\n",
    "        \n",
    "        # Log iteration\n",
    "        if global_step % LOG_EVERY_N_STEPS == 0:\n",
    "            log_data = monitor.log_iteration(\n",
    "                epoch=epoch + 1,\n",
    "                batch=batch_idx,\n",
    "                global_step=global_step,\n",
    "                loss=loss.item(),\n",
    "                lr=current_lr,\n",
    "                batch_size=batch_size,\n",
    "                seq_len=seq_len,\n",
    "                extra_metrics={\n",
    "                    'tokens_seen': global_step * batch_size * seq_len,\n",
    "                    'time_elapsed': (datetime.now() - epoch_start_time).total_seconds()\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'ppl': f\"{np.exp(loss.item()):.2f}\",\n",
    "            'lr': f\"{current_lr:.2e}\"\n",
    "        })\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\nRunning validation for epoch {epoch + 1}...\")\n",
    "    model_engine.eval()\n",
    "    val_loss_total = 0\n",
    "    val_steps = 0\n",
    "    \n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            inputs = batch['input_ids'].to(model_engine.device)\n",
    "            labels = inputs.clone()\n",
    "            \n",
    "            logits = model_engine(inputs)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, config.vocab_size), shift_labels.view(-1))\n",
    "            \n",
    "            val_loss_total += loss.item()\n",
    "            val_steps += 1\n",
    "            \n",
    "            val_progress_bar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss_total / val_steps\n",
    "    val_perplexity = np.exp(avg_val_loss)\n",
    "    \n",
    "    # Log validation results\n",
    "    monitor.log_validation(epoch + 1, avg_val_loss, val_perplexity)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation Perplexity: {val_perplexity:.2f}\")\n",
    "    print(f\"  Time Elapsed: {datetime.now() - epoch_start_time}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Update final plot with validation results\n",
    "    monitor.plot_metrics(update_frequency=1)\n",
    "\n",
    "# Training complete - show final statistics\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(\"\\nFinal Statistics:\")\n",
    "stats = monitor.get_summary_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLogs saved to: {monitor.log_dir}\")\n",
    "print(f\"  - JSON log: {monitor.json_log_path}\")\n",
    "print(f\"  - CSV metrics: {monitor.csv_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model checkpoint\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = f\"./checkpoints/{RUN_ID}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Saving final model to {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Save DeepSpeed checkpoint (includes model, optimizer, scheduler states)\n",
    "model_engine.save_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "# Save model configuration for easy reconstruction\n",
    "model_config = {\n",
    "    \"model_class\": \"CompressedMoEModel\",\n",
    "    \"vocab_size\": config.vocab_size,\n",
    "    \"d_model\": config.d_model,\n",
    "    \"num_layers\": config.num_layers,\n",
    "    \"num_heads\": config.num_heads,\n",
    "    \"num_dense_layers\": config.num_dense_layers,\n",
    "    \"num_moe_layers\": config.num_moe_layers,\n",
    "    \"num_experts\": config.num_experts,\n",
    "    \"d_latent\": config.d_latent,\n",
    "    \"d_intermediate_hypernet\": config.d_intermediate_hypernet,\n",
    "    \"top_k\": config.top_k,\n",
    "    \"num_routing_heads\": config.num_routing_heads,\n",
    "    \"d_query\": config.d_query,\n",
    "    \"d_ffn_intermediate\": config.d_ffn_intermediate,\n",
    "    \"max_seq_len\": config.max_seq_len,\n",
    "    \"training_completed\": datetime.now().isoformat(),\n",
    "    \"final_stats\": monitor.get_summary_stats()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(CHECKPOINT_DIR, \"model_config.json\"), \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "\n",
    "# Save training history for analysis\n",
    "import pandas as pd\n",
    "history_df = pd.read_csv(monitor.csv_log_path)\n",
    "history_df.to_csv(os.path.join(CHECKPOINT_DIR, \"training_history.csv\"), index=False)\n",
    "\n",
    "print(f\"Model saved successfully!\")\n",
    "print(f\"Checkpoint includes:\")\n",
    "print(f\"  - Model weights\")\n",
    "print(f\"  - Optimizer state\") \n",
    "print(f\"  - Learning rate scheduler state\")\n",
    "print(f\"  - Model configuration\")\n",
    "print(f\"  - Training history\")\n",
    "print(f\"\\nTo load later:\")\n",
    "print(f\"  model_engine.load_checkpoint('{CHECKPOINT_DIR}')\")\n",
    "\n",
    "# Also create a \"latest\" symlink for convenience\n",
    "latest_link = \"./checkpoints/latest\"\n",
    "if os.path.exists(latest_link):\n",
    "    os.unlink(latest_link)\n",
    "os.symlink(os.path.abspath(CHECKPOINT_DIR), latest_link)\n",
    "print(f\"\\nCreated symlink at {latest_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Inference and Testing\n",
    "Now that the model is trained and saved, let's test it with text generation. We'll implement different sampling strategies and see how the model performs on various prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference with the Trained Model\n",
    "from typing import Optional, List\n",
    "\n",
    "class ModelInference:\n",
    "    \"\"\"Inference wrapper for the trained model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_engine, tokenizer, device='cuda'):\n",
    "        self.model = model_engine.module if hasattr(model_engine, 'module') else model_engine\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.pad_token_id = tokenizer.eot_token\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, \n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        temperature: float = 0.8,\n",
    "        top_k: Optional[int] = 50,\n",
    "        top_p: Optional[float] = 0.9,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        do_sample: bool = True,\n",
    "        seed: Optional[int] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Encode the prompt\n",
    "        input_ids = self.tokenizer.encode(prompt, allowed_special=\"all\")\n",
    "        input_ids = torch.tensor([input_ids], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        # Create attention mask (all 1s for the prompt)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Track generated tokens for repetition penalty\n",
    "        generated_tokens = []\n",
    "        \n",
    "        # Generate tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get model predictions\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Get logits for the last position\n",
    "            next_token_logits = outputs[0, -1, :]\n",
    "            \n",
    "            # Apply repetition penalty\n",
    "            if repetition_penalty != 1.0 and generated_tokens:\n",
    "                for token_id in set(generated_tokens):\n",
    "                    if next_token_logits[token_id] < 0:\n",
    "                        next_token_logits[token_id] *= repetition_penalty\n",
    "                    else:\n",
    "                        next_token_logits[token_id] /= repetition_penalty\n",
    "            \n",
    "            # Apply temperature\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None and top_k > 0:\n",
    "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p is not None and top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Shift the indices to the right to keep also the first token above the threshold\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample or take argmax\n",
    "            if do_sample:\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "            \n",
    "            # Add to generated tokens for repetition penalty\n",
    "            generated_tokens.append(next_token.item())\n",
    "            \n",
    "            # Append to input\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=self.device, dtype=torch.long)], dim=1)\n",
    "            \n",
    "            # Stop if we hit the end token\n",
    "            if next_token.item() == self.pad_token_id:\n",
    "                break\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_ids = input_ids[0].tolist()\n",
    "        generated_text = self.tokenizer.decode(generated_ids)\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def generate_batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_new_tokens: int = 100,\n",
    "        temperature: float = 0.8,\n",
    "        **kwargs\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate text for multiple prompts\"\"\"\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            result = self.generate(prompt, max_new_tokens, temperature, **kwargs)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "# Initialize inference wrapper\n",
    "print(\"Initializing inference wrapper...\")\n",
    "inference = ModelInference(model_engine, enc, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with some example prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In a world where technology\",\n",
    "    \"The most important scientific discovery\",\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The key to successful machine learning is\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING MODEL INFERENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with different sampling strategies\n",
    "for i, prompt in enumerate(test_prompts[:3]):  # Test first 3 prompts\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    \n",
    "    # Greedy decoding\n",
    "    print(\"\\n[Greedy Decoding]\")\n",
    "    output = inference.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        seed=42\n",
    "    )\n",
    "    print(output)\n",
    "    \n",
    "    # Sampling with temperature\n",
    "    print(\"\\n[Sampling with Temperature=0.8]\")\n",
    "    output = inference.generate(\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        seed=42\n",
    "    )\n",
    "    print(output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Post-Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAnalyzer:\n",
    "    \"\"\"Analyze training logs after completion\"\"\"\n",
    "    \n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        self.df = None\n",
    "        self.load_logs()\n",
    "    \n",
    "    def load_logs(self):\n",
    "        \"\"\"Load training logs into a DataFrame\"\"\"\n",
    "        logs = []\n",
    "        with open(self.log_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    log = json.loads(line.strip())\n",
    "                    if log.get('type') != 'validation':  # Skip validation entries\n",
    "                        logs.append(log)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        self.df = pd.DataFrame(logs)\n",
    "        if 'timestamp' in self.df.columns:\n",
    "            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
    "    \n",
    "    def plot_training_curves(self, smooth_window=100):\n",
    "        \"\"\"Plot detailed training curves\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss curve with smoothing\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(self.df['global_step'], self.df['loss'], alpha=0.3, label='Raw')\n",
    "        if len(self.df) > smooth_window:\n",
    "            smoothed = self.df['loss'].rolling(window=smooth_window).mean()\n",
    "            ax.plot(self.df['global_step'], smoothed, label=f'Smoothed ({smooth_window})')\n",
    "        ax.set_xlabel('Global Step')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('Training Loss Over Time')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(self.df['global_step'], self.df['lr'])\n",
    "        ax.set_xlabel('Global Step')\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.set_title('Learning Rate Schedule')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Perplexity distribution\n",
    "        ax = axes[1, 0]\n",
    "        valid_ppl = self.df[self.df['perplexity'] < 1000]['perplexity']\n",
    "        ax.hist(valid_ppl, bins=50, alpha=0.7)\n",
    "        ax.set_xlabel('Perplexity')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Perplexity Distribution')\n",
    "        ax.axvline(valid_ppl.median(), color='red', linestyle='--', \n",
    "                   label=f'Median: {valid_ppl.median():.2f}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Training speed\n",
    "        ax = axes[1, 1]\n",
    "        if 'time_elapsed' in self.df.columns:\n",
    "            steps_per_second = self.df['global_step'] / self.df['time_elapsed']\n",
    "            ax.plot(self.df['global_step'], steps_per_second)\n",
    "            ax.set_xlabel('Global Step')\n",
    "            ax.set_ylabel('Steps/Second')\n",
    "            ax.set_title('Training Speed')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_best_checkpoint(self):\n",
    "        \"\"\"Find the step with the best (lowest) average loss\"\"\"\n",
    "        window = min(100, len(self.df) // 10)\n",
    "        self.df['avg_loss'] = self.df['loss'].rolling(window=window).mean()\n",
    "        best_idx = self.df['avg_loss'].idxmin()\n",
    "        return self.df.loc[best_idx]\n",
    "\n",
    "# After training, analyze the results\n",
    "analyzer = TrainingAnalyzer(monitor.json_log_path)\n",
    "print(f\"Loaded {len(analyzer.df)} training steps\")\n",
    "\n",
    "# Show training curves\n",
    "analyzer.plot_training_curves()\n",
    "\n",
    "# Find best checkpoint\n",
    "best = analyzer.get_best_checkpoint()\n",
    "print(f\"\\nBest checkpoint:\")\n",
    "print(f\"  Step: {best['global_step']}\")\n",
    "print(f\"  Loss: {best['loss']:.4f}\")\n",
    "print(f\"  Perplexity: {best['perplexity']:.2f}\")\n",
    "print(f\"  Epoch: {best['epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_training_summary(monitor, output_file=\"training_summary.json\"):\n",
    "    \"\"\"Export a comprehensive training summary\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'run_id': monitor.run_id,\n",
    "        'start_time': datetime.fromtimestamp(os.path.getctime(monitor.json_log_path)).isoformat(),\n",
    "        'end_time': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'model_params': total_params,\n",
    "            'active_params': total_active,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate': ds_config['optimizer']['params']['lr'],\n",
    "            'num_epochs': NUM_EPOCHS,\n",
    "            'vocab_size': config.vocab_size,\n",
    "            'd_model': config.d_model,\n",
    "            'num_layers': config.num_layers,\n",
    "            'num_experts': config.num_experts,\n",
    "            'top_k': config.top_k\n",
    "        },\n",
    "        'results': monitor.get_summary_stats(),\n",
    "        'validation_history': monitor.val_history,\n",
    "        'log_files': {\n",
    "            'json_log': monitor.json_log_path,\n",
    "            'csv_metrics': monitor.csv_log_path\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Training summary exported to: {output_file}\")\n",
    "    return summary\n",
    "\n",
    "# Export summary\n",
    "summary = export_training_summary(monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "If the validation perplexity consistently decreases after each epoch, the experiment is a success. It demonstrates that the core concept of generating expert weights on-the-fly from a compressed latent space is a viable and trainable architectural strategy. The multi-head routing mechanism from the PEER paper ensures diverse expert selection while maintaining computational efficiency through product-key decomposition.\n",
    "\n",
    "The reordered execution pattern provides significant performance improvements:\n",
    "- ~8x faster execution based on benchmarks\n",
    "- ~4x reduction in working memory per token\n",
    "- Better cache utilization through improved data locality\n",
    "\n",
    "The training logs and visualizations provide comprehensive insights into the model's learning dynamics, including:\n",
    "- Real-time loss and perplexity tracking\n",
    "- Learning rate schedules\n",
    "- GPU memory usage patterns\n",
    "- Training speed metrics\n",
    "\n",
    "All metrics are saved in both JSON and CSV formats for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
